# -*- coding: utf-8 -*-
"""University Admission Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CoN2azVJnuDsuJVQ9TboAN67TyOK1iuw
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import pickle
warnings.filterwarnings('ignore')
data=pd.read_csv('/content/drive/MyDrive/Project.csv')
data.info()
data.isnull().any()

data.head()

data.describe()

sns.distplot(data['GRE Score'])

from google.colab import drive
drive.mount('/content/drive')

plt.figure(figsize=(10,7))
sns.heatmap(data.corr(),annot=True)

sns.pairplot(data=data,hue='Research',markers=["^","v"],palette='inferno')

sns.scatterplot(x='University Rating',y='CGPA',data=data,color='r',s=100)

category=['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']
color=['yellowgreen','gold','lightskyblue','pink','red','purple','orange','gray']
start=True
for i in np.arange(4):
  fig=plt.figure(figsize=(14,8))
  plt.subplot2grid((4,2),(i,0))
  data[category[2*i]].hist(color=color[2*i],bins=10)
  plt.title(category[2*i])
  plt.subplot2grid((4,2),(i,1))
  data[category[2*i+1]].hist(color=color[2*i+1],bins=10)
  plt.title(category[2*i+1])

from sklearn.preprocessing import MinMaxScaler
x=data.iloc[:,:7].values
y=data.iloc[:,7].values
sc=MinMaxScaler()
x=sc.fit_transform(x)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=101)
y_train=(y_train>0.5)
y_train
y_test=(y_test>0.5)
y_test
from sklearn.linear_model import LogisticRegression
cls=LogisticRegression()
Ir=cls.fit(x_train,y_train)
y_pred=Ir.predict(x_test)
y_pred

print(x_train.shape)

#libraries to train Neural network
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense,Activation,Dropout
from tensorflow.keras.optimizers import Adam
#Initialize the model
model=keras.Sequential()
#Add input layer
model.add(Dense(7,activation='relu',input_dim=7))
#Add hidden layers
model.add(Dense(7,activation='relu'))
#Add output layer
model.add(Dense(1,activation='linear'))
model.summary()

model.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy'])
model.fit(x_train,y_train,batch_size=20,epochs=30)

from sklearn.metrics import accuracy_score
#Make predications on the training data
train_predications=model.predict(x_train)
print(train_predications)

#get the training accuracy
train_acc=model.evaluate(x_train,y_train,verbose=0)[1]
print(train_acc)

#get the test accuracy
test_acc=model.evaluate(x_test,y_test,verbose=0)[1]
print(test_acc)

x=data.head()
x

x

pred=model.predict(x_test)
pred=(pred>0.5)
pred

from sklearn.metrics import accuracy_score,recall_score,roc_auc_score,confusion_matrix
print("Accuracy score:%f"% (accuracy_score(y_test,y_pred)*100))
print("Recall score:%f"% (recall_score(y_test,y_pred)*100))
print("ROC score %f\n"%(roc_auc_score(y_test,y_pred)*100))
print(confusion_matrix(y_test,y_pred))

from sklearn.metrics import accuracy_score,recall_score,roc_auc_score,confusion_matrix,classification_report
print(classification_report(y_test,pred))

with open('university.pkl','wb') as f:
  pickle.dump(model,f)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
data=pd.read_csv('/content/drive/MyDrive/Project.csv')
data.info()
data.isnull().any()
data.head()
data.describe()
sns.distplot(data['GRE Score'])
plt.figure(figsize=(10,7))
sns.heatmap(data.corr(),annot=True)
sns.pairplot(data=data,hue='Research',markers=["^","v"],palette='inferno')
sns.scatterplot(x='University Rating',y='CGPA',data=data,color='r',s=100)
category=['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']
color=['yellowgreen','gold','lightskyblue','pink','red','purple','orange','gray']
start=True
for i in np.arange(4):
  fig=plt.figure(figsize=(14,8))
  plt.subplot2grid((4,2),(i,0))
  data[category[2*i]].hist(color=color[2*i],bins=10)
  plt.title(category[2*i])
  plt.subplot2grid((4,2),(i,1))
  data[category[2*i+1]].hist(color=color[2*i+1],bins=10)
  plt.title(category[2*i+1])
  from sklearn.preprocessing import MinMaxScaler
x=data.iloc[:,:7].values
y=data.iloc[:,7].values
sc=MinMaxScaler()
x=sc.fit_transform(x)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=101)
y_train=(y_train>0.5)
y_train
y_test=(y_test>0.5)
y_test
from sklearn.linear_model import LogisticRegression
cls=LogisticRegression()
Ir=cls.fit(x_train,y_train)
y_pred=Ir.predict(x_test)
y_pred
#libraries to train Neural network
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense,Activation,Dropout
from tensorflow.keras.optimizers import Adam
#Initialize the model
model=keras.Sequential()
#Add input layer
model.add(Dense(7,activation='relu',input_dim=7))
#Add hidden layers
model.add(Dense(7,activation='relu'))
#Add output layer
model.add(Dense(1,activation='linear'))
model.summary()
model.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy'])
model.fit(x_train,y_train,batch_size=20,epochs=30)
from sklearn.metrics import accuracy_score
#Make predications on the training data
train_predications=model.predict(x_train)
print(train_predications)
#get the training accuracy
train_acc=model.evaluate(x_train,y_train,verbose=0)[1]
print(train_acc)
#get the test accuracy
test_acc=model.evaluate(x_test,y_test,verbose=0)[1]
print(test_acc)
pred=model.predict(x_test)
pred=(pred>0.5)
pred
from sklearn.metrics import accuracy_score,recall_score,roc_auc_score,confusion_matrix
print("Accuracy score:%f"% (accuracy_score(y_test,y_pred)*100))
print("Recall score:%f"% (recall_score(y_test,y_pred)*100))
print("ROC score %f\n"%(roc_auc_score(y_test,y_pred)*100))
print(confusion_matrix(y_test,y_pred))
from sklearn.metrics import accuracy_score,recall_score,roc_auc_score,confusion_matrix,classification_report
print(classification_report(y_train,pred))
from sklearn.metrics import accuracy_score,recall_score,roc_auc_score,confusion_matrix,classification_report
print(classification_report(y_test,pred))

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

data=pd.read_csv('/content/drive/MyDrive/liver_patient.csv')

data.head()

data.info()

data.isnull().any()

data.isnull().sum()

from sklearn.preprocessing import LabelEncoder
Ic=LabelEncoder()
data['Gender']=Ic.fit_transform(data['Gender'])

data.describe()

sns.distplot(data['age'])
plt.title('Age Distribution Graph')
plt.show()

sns.countplot(x='outcome',hue='Gender',data=data)

plt.figure(figsize=(10,7))
sns.heatmap(data.corr(),annot=True)

X=data.head()
X

from sklearn.preprocessing import scale
X_scaled=pd.DataFrame(scale(X),columns=X.columns)
X_scaled.head()

X=data.iloc[:,:-1]
y=data.outcome
from sklearn.model_selection import train_test_split
X_train,x_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.2,random_state=42)

pip install imblearn

from imblearn.over_sampling import SMOTE
smote=SMOTE()

y_train.value_counts()

X_train = pd.DataFrame(X_train).dropna()
y_train = y_train[X_train.index]
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
print(y_train_smote.value_counts())

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_train_smote, y_train_smote, test_size=0.2, random_state=42)
from sklearn.ensemble import RandomForestClassifier
model1 = RandomForestClassifier()
model1.fit(X_train, y_train)
y_predict = model1.predict(X_test)
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
rfc1 = accuracy_score(y_test, y_predict)
print("Accuracy:", rfc1)
print("Classification Report:\n", classification_report(y_test, y_predict))

from sklearn.tree import DecisionTreeClassifier
model4=DecisionTreeClassifier()
model4.fit(X_train_smote, y_train_smote)
y_predict=model4.predict(X_test)
dtc1=accuracy_score(y_test, y_predict)
dtc1
pd.crosstab(y_test, y_predict)
print(classification_report(y_test, y_predict))

from sklearn.neighbors import KNeighborsClassifier
model2=KNeighborsClassifier()
model2.fit(X_train_smote, y_train_smote)
y_predict=model2.predict(X_test)
knn1=(accuracy_score(y_test, y_predict))
knn1
pd.crosstab(y_test, y_predict)
print(classification_report(y_test, y_predict))

from sklearn.linear_model import LogisticRegression
model5=LogisticRegression()
model5.fit(X_train_smote, y_train_smote)
y_predict=model5.predict(X_test)
logi1=(accuracy_score(y_test, y_predict))
logi1
pd.crosstab(y_test, y_predict)
print(classification_report(y_test, y_predict))

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
classifier=Sequential()
classifier.add(Dense(units=100, activation='relu',input_dim=10))
classifier.add(Dense(units=50, activation='relu'))
classifier.add(Dense(units=1, activation='sigmoid'))
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_history= classifier.fit(X_train, y_train ,batch_size=100,epochs=100)

model4.predict([[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]])
model1.predict([[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]])

classifier.save("liver.h5")
y_pred=classifier.predict(X_test)
y_pred
y_pred=(y_pred > 0.5)
y_pred

def predict_exit(sample_value):

	
	 sample_value= np.array(sample_value)
	
	 sample_value=sample_value.reshape(1,-1)
	
	 sample_value=scale(sample_value)
	 return classifier.predict(sample_value)

sample_value=[[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]]
if predict_exit(sample_value)>0.5:
	print('Prediction: Liver Patient')
else:
	print('Prediction: Healthy')

acc_smote= [['KNN Classifier', knn1], ['RandomForestClassifier', rfc1],
             ['DecisionTreeClassifier', dtc1], ['LogisticRegression',logi1]]
Liverpatient_pred=pd.DataFrame(acc_smote, columns=['classification models', 'accuracy_score'])
Liverpatient_pred

plt.figure(figsize=(7,5))
plt.xticks(rotation=90)
plt.title('Classification models & accuracy scores after SMOTE', fontsize=18)
sns.barplot(x="classification models", y="accuracy_score", data=Liverpatient_pred,palette="Set2")

from sklearn.ensemble import ExtraTreesClassifier
model=ExtraTreesClassifier()
model.fit(X,y)

model.feature_importances_
dd=pd.DataFrame(model.feature_importances_,index=X.columns).sort_values(0,ascending=False)
dd
dd.plot(kind='barh', figsize=(7,6))
plt.title("FEATURE IMPORTANCE", fontsize=14)